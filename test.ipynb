{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00522767",
   "metadata": {},
   "source": [
    "# RAG System Implementation with Qwen Models\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) system using Qwen models. The implementation includes:\n",
    "\n",
    "1. Basic Single-Turn RAG\n",
    "2. Feature A: Multi-Turn Search (Optional)\n",
    "3. Feature B: Agentic Workflow (Optional)\n",
    "\n",
    "We'll be using one of the following Qwen models:\n",
    "- Qwen2.5-0.5B-Instruct\n",
    "- Qwen2.5-1.5B-Instruct\n",
    "- Qwen2.5-3B-Instruct\n",
    "- Qwen2.5-7B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb3c00",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "Let's start by installing and importing the required libraries. We'll need:\n",
    "- transformers: For the Qwen model\n",
    "- sentence-transformers: For text embeddings\n",
    "- faiss-cpu/faiss-gpu: For vector storage and similarity search\n",
    "- torch: For deep learning operations\n",
    "- Additional utilities for text processing and data management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a04112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers sentence-transformers faiss-cpu numpy pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4da5d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bee/miniforge3/envs/qwenrag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Dict' from 'types' (/home/bee/miniforge3/envs/qwenrag/lib/python3.12/types.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BasicRAG, MultiTurnRAG\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document, load_documents\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# set cache\u001b[39;00m\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Set device\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/qwenRAG/rag.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdocument\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrecorder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Recorder\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBasicRAG\u001b[39;00m:\n\u001b[32m      6\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Basic RAG system based on context retrived with functions that build prompt and generate response\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/qwenRAG/recorder.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deque\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRecorder\u001b[39;00m:\n\u001b[32m      5\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Record last 5 conversations\"\"\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'Dict' from 'types' (/home/bee/miniforge3/envs/qwenrag/lib/python3.12/types.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rag import BasicRAG, MultiTurnRAG\n",
    "from data.document import Document, load_documents\n",
    "# set cache\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize Qwen model and tokenizer (we'll use the smallest model for demonstration)\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Initialize sentence transformer for embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098a6f8b",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "In this section, we'll implement the following components:\n",
    "1. Text chunking function to split documents into manageable pieces\n",
    "2. Data loading utilities\n",
    "3. Text preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6fb373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 document chunks\n"
     ]
    }
   ],
   "source": [
    "# @dataclass\n",
    "# class Document:\n",
    "#     \"\"\"Class to represent a document chunk.\"\"\"\n",
    "#     text: str\n",
    "#     metadata: Dict[str, Any]\n",
    "\n",
    "# def split_text(text: str, chunk_size: int = 200, overlap: int = 50) -> List[str]:\n",
    "#     \"\"\"Split text into chunks with overlap.\"\"\"\n",
    "#     words = text.split()\n",
    "#     chunks = []\n",
    "    \n",
    "#     for i in range(0, len(words), chunk_size - overlap):\n",
    "#         chunk = ' '.join(words[i:i + chunk_size])\n",
    "#         chunks.append(chunk)\n",
    "    \n",
    "#     return chunks\n",
    "\n",
    "# def load_documents(directory: str) -> List[Document]:\n",
    "#     \"\"\"Load documents from a directory.\"\"\"\n",
    "#     documents = []\n",
    "    \n",
    "#     # This is a placeholder - implement actual document loading based on your data\n",
    "#     # For demonstration, we'll create some sample documents\n",
    "#     sample_texts = [\n",
    "#         \"Barack Obama was born in Hawaii. He served as the 44th president of the United States.\",\n",
    "#         \"Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\",\n",
    "#         \"Barack and Michelle Obama met in Chicago and got married in 1992.\",\n",
    "#         \"Kathy lives in Lai King\"\n",
    "#     ]\n",
    "    \n",
    "#     for i, text in enumerate(sample_texts):\n",
    "#         chunks = split_text(text)\n",
    "#         for chunk in chunks:\n",
    "#             doc = Document(\n",
    "#                 text=chunk,\n",
    "#                 metadata={'source_id': i, 'source_text': text}\n",
    "#             )\n",
    "#             documents.append(doc)\n",
    "    \n",
    "#     return documents\n",
    "\n",
    "# Load documents\n",
    "documents = load_documents(\"path/to/your/documents\")\n",
    "print(f\"Loaded {len(documents)} document chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf36d6a",
   "metadata": {},
   "source": [
    "## 3. Vector Store Creation\n",
    "\n",
    "Now we'll create a FAISS vector store to index our document embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936403b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test search results:\n",
      "- Barack Obama was born in Hawaii. He served as the 44th president of the United States.\n",
      "- Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\n",
      "- Barack and Michelle Obama met in Chicago and got married in 1992.\n"
     ]
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, embedding_dim: int = 384):  # MiniLM-L6-v2 has 384 dimensions\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        self.documents = []\n",
    "        \n",
    "    def add_documents(self, documents: List[Document]):\n",
    "        \"\"\"Add documents to the vector store.\"\"\"\n",
    "        texts = [doc.text for doc in documents]\n",
    "        embeddings = embedding_model.encode(texts, convert_to_tensor=True)\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "        \n",
    "        self.index.add(embeddings)\n",
    "        self.documents.extend(documents)\n",
    "    \n",
    "    def search(self, query: str, k: int = 3) -> List[Document]:\n",
    "        \"\"\"Search for similar documents.\"\"\"\n",
    "        query_embedding = embedding_model.encode([query], convert_to_tensor=True)\n",
    "        query_embedding = query_embedding.cpu().numpy()\n",
    "        \n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        return [self.documents[i] for i in indices[0]]\n",
    "\n",
    "# Create vector store and add documents\n",
    "vector_store = VectorStore()\n",
    "vector_store.add_documents(documents)\n",
    "\n",
    "# Test search\n",
    "results = vector_store.search(\"Where was Barack Obama born?\")\n",
    "print(\"Test search results:\")\n",
    "for doc in results:\n",
    "    print(f\"- {doc.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16b812",
   "metadata": {},
   "source": [
    "## 4. Basic RAG Implementation\n",
    "\n",
    "Now we'll implement the core RAG pipeline components:\n",
    "1. Passage retrieval\n",
    "2. Prompt construction\n",
    "3. Response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a9a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: where did Kathy live\n",
      "Answer: Lai King.\n"
     ]
    }
   ],
   "source": [
    "# class BasicRAG:\n",
    "#     def __init__(self, model, tokenizer, vector_store):\n",
    "#         self.model = model\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.vector_store = vector_store\n",
    "#         self.assistant_instruction = \"\"\"You are a helpful assistant. Reply briefly and only with information explicitly stated in the context that directly answers the question. If you cannot find an answer based on the provided information, say \"I don't have enough information to answer that.\". Use the following examples as a guide for your response style:\n",
    "\n",
    "#         Example 1:\n",
    "#         Context: The capital of Germany is the city of Berlin. The capital of France is Paris.\n",
    "#         Query: What is the capital of Germany?\n",
    "#         Answer: The capital of Germany is Berlin.\n",
    "\n",
    "#         Example 2:\n",
    "#         Context: William Shakespeare was an English playwright. He wrote Romeo and Juliet in the 1590s.\n",
    "#         Query: Who wrote Romeo and Juliet?\n",
    "#         Answer: Romeo and Juliet was written by William Shakespeare.\n",
    "\n",
    "#         Example 3:\n",
    "#         Context: BeiJing is in China.\n",
    "#         Query: Where is New York?\n",
    "#         Answer: I don't have enough information to answer that.\n",
    "\n",
    "#         Now, use this context to answer the query:\"\"\"\n",
    "        \n",
    "#     def _build_prompt(self, query: str, contexts: List[Document]) -> str:\n",
    "#         \"\"\"Build a prompt combining the query and retrieved contexts.\"\"\"\n",
    "#         context_str = \"\\n\\n\".join([doc.text for doc in contexts])\n",
    "        \n",
    "#         prompt = f\"\"\"{self.assistant_instruction}\n",
    "#         Context:\n",
    "#         {context_str}\n",
    "\n",
    "#         Query: {query}\n",
    "\n",
    "#         Answer:\"\"\"\n",
    "        \n",
    "#         return prompt\n",
    "    \n",
    "#     def generate_response(self, query: str, max_new_tokens: int = 200) -> str:\n",
    "#         \"\"\"Generate a response using RAG.\"\"\"\n",
    "#         # Retrieve relevant documents\n",
    "#         relevant_docs = self.vector_store.search(query)\n",
    "        \n",
    "#         # Build prompt\n",
    "#         prompt = self._build_prompt(query, relevant_docs)\n",
    "        \n",
    "#         # Generate response\n",
    "#         inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "#         outputs = self.model.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens=max_new_tokens,\n",
    "#             num_return_sequences=1,\n",
    "#             temperature=0.3, # less creative answer\n",
    "#             pad_token_id=self.tokenizer.eos_token_id \n",
    "#         )\n",
    "        \n",
    "#         response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#         return response.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# Initialize RAG system\n",
    "rag_system = BasicRAG(model, tokenizer, vector_store)\n",
    "\n",
    "# Test the basic RAG system\n",
    "test_query = \"where did Kathy live\"\n",
    "response = rag_system.generate_response(test_query)\n",
    "print(f\"Question: {test_query}\")\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c3b0e",
   "metadata": {},
   "source": [
    "## 6. Multi-Turn Conversation Handler (Feature A)\n",
    "\n",
    "Implement the multi-turn conversation feature with context management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1b70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing multi-turn conversation:\n",
      "\n",
      "Q: Where was Barack Obama born?\n",
      "A: Barack Obama was born in Hawaii.\n",
      "\n",
      "Q: What about his wife, where was she born?\n",
      "A: Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\n",
      "\n",
      "Q: Where does kathy live\n",
      "A: Kathy lives in Lai King.\n",
      "\n",
      "Q: Where was Barack Obama's wife's husband born\n",
      "A: Michelle Obama's husband was born in Chicago, Illinois. Therefore, Barack Obama's wife, Michelle Obama, was born in Chicago, Illinois.\n"
     ]
    }
   ],
   "source": [
    "# Initialize multi-turn RAG system\n",
    "multi_turn_rag = MultiTurnRAG(model, tokenizer, vector_store)\n",
    "\n",
    "# Test multi-turn conversation\n",
    "queries = [\n",
    "    \"Where was Barack Obama born?\",\n",
    "    \"What about his wife, where was she born?\",\n",
    "]\n",
    "\n",
    "print(\"Testing multi-turn conversation:\")\n",
    "for query in queries:\n",
    "    response = multi_turn_rag.generate_response(query)\n",
    "    print(f\"\\nQ: {query}\")\n",
    "    print(f\"A: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165fd767",
   "metadata": {},
   "source": [
    "## 7. Agentic Workflow Components (Feature B)\n",
    "\n",
    "Implement advanced features for improved response quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fcc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticRAG(MultiTurnRAG):\n",
    "    def __init__(self, model, tokenizer, vector_store):\n",
    "        super().__init__(model, tokenizer, vector_store)\n",
    "    \n",
    "    def _decompose_query(self, query: str) -> List[str]:\n",
    "        \"\"\"Decompose complex queries into simpler sub-queries.\"\"\"\n",
    "        decomposition_prompt = f\"\"\"Break down the following question into simpler sub-questions:\n",
    "        Question: {query}\n",
    "        \n",
    "        Generate sub-questions in a numbered list. Keep it simple.\"\"\"\n",
    "        \n",
    "        inputs = self.tokenizer(decomposition_prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=256)\n",
    "        decomposition = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract sub-questions (simple implementation)\n",
    "        sub_queries = [\n",
    "            line.split(\". \", 1)[1] if \". \" in line else line\n",
    "            for line in decomposition.split(\"\\n\")\n",
    "            if line.strip() and line[0].isdigit()\n",
    "        ]\n",
    "        \n",
    "        return sub_queries\n",
    "    \n",
    "    def _self_check(self, response: str, context: List[Document]) -> bool:\n",
    "        \"\"\"Verify if the response is supported by the context.\"\"\"\n",
    "        context_str = \"\\n\".join([doc.text for doc in context])\n",
    "        \n",
    "        verification_prompt = f\"\"\"Given the following context and response, determine if the response is fully supported by the context.\n",
    "        \n",
    "        Context: {context_str}\n",
    "        \n",
    "        Response: {response}\n",
    "        \n",
    "        Is the response fully supported by the context? Answer with just 'yes' or 'no'.\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        \n",
    "        inputs = self.tokenizer(verification_prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=1,temperature=0.1)\n",
    "        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True).lower()\n",
    "        \n",
    "        return result.split(\"Answer:\")[-1].strip()\n",
    "    \n",
    "    def generate_response(self, query: str, max_new_tokens: int = 600) -> str:\n",
    "        \"\"\"Generate a response using the agentic workflow.\"\"\"\n",
    "        # 1. Query Planning\n",
    "        sub_queries = self._decompose_query(query)\n",
    "        print(sub_queries)\n",
    "        # 2. Parallel Retrieval and Response Generation\n",
    "        sub_responses = []\n",
    "        for sub_query in sub_queries:\n",
    "            # Get relevant documents for sub-query\n",
    "            relevant_docs = self.vector_store.search(sub_query)\n",
    "            \n",
    "            # Generate response for sub-query\n",
    "            prompt = self._build_prompt(sub_query, relevant_docs, include_history=False)\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            outputs = self.model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=0.5,\n",
    "                do_sample=True, \n",
    "                 pad_token_id=self.tokenizer.eos_token_id \n",
    "            )\n",
    "            sub_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            sub_responses.append(sub_response)\n",
    "        \n",
    "        # 3. Response Synthesis\n",
    "        synthesis_prompt = f\"\"\"Synthesize a coherent answer from the following responses:\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Sub-responses:\n",
    "        {chr(10).join(sub_responses)}\n",
    "        \n",
    "        Provide a clear, concise answer that addresses the original question.\"\"\"\n",
    "\n",
    "        print(synthesis_prompt)\n",
    "        \n",
    "        inputs = self.tokenizer(synthesis_prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        final_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 4. Verification\n",
    "        relevant_docs = self.vector_store.search(query)\n",
    "        if not self._verify_response(final_response, relevant_docs):\n",
    "            final_response += \"\\n[Note: Some details in this response may require verification.]\"\n",
    "        \n",
    "        # Update conversation history\n",
    "        self.recorder.append_history(query, final_response)\n",
    "        \n",
    "        return final_response\n",
    "\n",
    "# Initialize agentic RAG system\n",
    "agentic_rag = AgenticRAG(model, tokenizer, vector_store)\n",
    "\n",
    "# Test complex query\n",
    "complex_query = \"Tell me about Barack Obama's early life and his wife's background.\"\n",
    "response = agentic_rag.generate_response(complex_query)\n",
    "# print(f\"\\nComplex Query: {complex_query}\")\n",
    "# print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21868f6d",
   "metadata": {},
   "source": [
    "## 8. System Integration\n",
    "\n",
    "Let's create a unified interface that allows switching between different RAG modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e99b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different RAG modes:\n",
      "\n",
      "Mode: basic\n",
      "Q: Where was Barack Obama born?\n",
      "A: According to the given context, Barack Obama was born in Hawaii. The information states:\n",
      "\n",
      "\"Barack Obama was born in Hawaii. He served as the 44th president of the United States.\"\n",
      "\n",
      "So, the correct answer is: Hawaii.\n",
      "I don't have enough information to answer that.\n",
      "Therefore, the response is: I don't have enough information to answer that.\n",
      "\n",
      "Mode: multi_turn\n",
      "Q: What about his wife?\n",
      "A: The passage states that Michelle Obama was born in Chicago, Illinois and she served as First Lady of the United States. It also mentions that Barack Obama was born in Hawaii and he served as the 44th president of the United States. Therefore, it is not possible to determine what about his wife from the given information. I don't have enough information to answer that question.\n",
      "Therefore, my answer is: I don't have enough information to answer that.\n",
      "\n",
      "Mode: agentic\n",
      "Q: Tell me about both Barack and Michelle Obama's early lives.\n",
      "A: Synthesize a coherent answer from the following responses:\n",
      "\n",
      "        Question: Tell me about both Barack and Michelle Obama's early lives.\n",
      "\n",
      "        Sub-responses:\n",
      "        You are a helpful assistant. Use the following information to answer the question.\n",
      "        If you cannot find the answer in the provided context, say \"I don't have enough information to answer that.\"\n",
      "\n",
      "        Context:\n",
      "        Barack Obama was born in Hawaii. He served as the 44th president of the United States.\n",
      "\n",
      "Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\n",
      "\n",
      "Barack and Michelle Obama met in Chicago and got married in 1992.\n",
      "        \n",
      "        Question: Describe your family\n",
      "\n",
      "        Answer: Let me answer based on the provided information. I don't have enough information to answer this question about my family. The given context only provides information about Barack Obama and Michelle Obama's birthplaces and their roles as presidents of the United States. There is no information about my family or any other aspects of it. Therefore, I am unable to provide an accurate description of my family based solely on the provided context. To answer this specific question, additional information would be needed. Can you please provide more details about who you are? I'm here to help with any questions you might have! #familyinformationrequired\n",
      "In summary, the question requires information not available in the provided context. It is necessary to ask for personal information such as name, age, place of birth, etc., to provide a proper response. As requested, I can now proceed to answer the question using the provided context.\n",
      "\n",
      "You are a helpful assistant. Use the following information to answer the question.\n",
      "        If you cannot find the answer in the provided context, say \"I don't have enough information to answer that.\"\n",
      "\n",
      "        Context:\n",
      "        Barack and Michelle Obama met in Chicago and got married in 1992.\n",
      "\n",
      "Barack Obama was born in Hawaii. He served as the 44th president of the United States.\n",
      "\n",
      "Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\n",
      "        \n",
      "        Question: Explain how you got involved with politics\n",
      "\n",
      "        Answer: Let me answer based on the provided information. I don't have enough information to directly answer this question about how I got involved with politics.\n",
      "        Information used: The given context provides biographical details about Barack Obama and Michelle Obama, including their birthplaces and political roles. However, it does not provide any information about my involvement in politics or how I became interested in politics.\n",
      "        Response: In order to answer this question, additional information about my personal experiences and interests would be necessary. Without this type of information, it is impossible for me to accurately answer whether I got involved with politics or not. The available information only tells us about my family's background and my role in the political process, but not about my own political beliefs or actions. Therefore, I do not have sufficient information to answer this specific question about my involvement in politics. To provide an accurate response, I would need more specific details about my personal history and political activities. Please let me know if there is anything else I can help you with! #PoliticalAwareness\n",
      "In summary, while I do not have enough information to directly answer the question about my involvement with politics, I am aware of my family's background and political experience. My goal is to provide accurate and relevant information to those who may be interested in learning more about my life and career. Is there anything else I can assist you with? #PoliticalAwareness\n",
      "\n",
      "You are a helpful assistant. Use the following information to answer the question.\n",
      "        If you cannot find the answer in the provided context, say \"I don't have enough information to answer that.\"\n",
      "\n",
      "        Context:\n",
      "        Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\n",
      "\n",
      "Barack Obama was born in Hawaii. He served as the 44th president of the United States.\n",
      "\n",
      "Barack and Michelle Obama met in Chicago and got married in 1992.\n",
      "        \n",
      "        Question: What else did you do before becoming President?\n",
      "\n",
      "        Answer: Let me answer based on the provided information. Barack Obama did not serve as the 44th president of the United States. Therefore, he had nothing else to do before becoming President. I don't have enough information to answer that question. \n",
      "\n",
      "So the final answer is: I don't have enough information to answer that.\n",
      "\n",
      "        Provide a clear, concise answer that addresses the original question. The answer should include all the key points from the provided information, but also avoid repeating information already present in the text. The answer should be grammatically correct and easy to understand. #Obama\n"
     ]
    }
   ],
   "source": [
    "class UnifiedBasicRAG:\n",
    "    def __init__(self, model_name: str = \"Qwen/Qwen2.5-0.5B-Instruct\"):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.vector_store = VectorStore()\n",
    "        \n",
    "        # Initialize different RAG modes\n",
    "        self.basic_rag = BasicRAG(self.model, self.tokenizer, self.vector_store)\n",
    "        self.multi_turn_rag = MultiTurnRAG(self.model, self.tokenizer, self.vector_store)\n",
    "        self.agentic_rag = AgenticRAG(self.model, self.tokenizer, self.vector_store)\n",
    "        \n",
    "    def add_documents(self, documents: List[Document]):\n",
    "        \"\"\"Add documents to the knowledge base.\"\"\"\n",
    "        self.vector_store.add_documents(documents)\n",
    "    \n",
    "    def query(self, \n",
    "             query: str, \n",
    "             mode: str = \"basic\",\n",
    "             max_new_tokens: int = 200) -> str:\n",
    "        \"\"\"\n",
    "        Query the RAG system using specified mode.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's question\n",
    "            mode: One of [\"basic\", \"multi_turn\", \"agentic\"]\n",
    "            max_new_tokens: Maximum length of the generated response\n",
    "        \"\"\"\n",
    "        if mode == \"basic\":\n",
    "            return self.basic_rag.generate_response(query, max_new_tokens)\n",
    "        elif mode == \"multi_turn\":\n",
    "            return self.multi_turn_rag.generate_response(query, max_new_tokens)\n",
    "        elif mode == \"agentic\":\n",
    "            return self.agentic_rag.generate_response(query, max_new_tokens)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "# Initialize unified system\n",
    "unified_rag = UnifiedBasicRAG()\n",
    "unified_rag.add_documents(documents)\n",
    "\n",
    "# Test different modes\n",
    "test_queries = [\n",
    "    (\"Where was Barack Obama born?\", \"basic\"),\n",
    "    (\"What about his wife?\", \"multi_turn\"),\n",
    "    (\"Tell me about both Barack and Michelle Obama's early lives.\", \"agentic\")\n",
    "]\n",
    "\n",
    "print(\"Testing different RAG modes:\")\n",
    "for query, mode in test_queries:\n",
    "    print(f\"\\nMode: {mode}\")\n",
    "    print(f\"Q: {query}\")\n",
    "    response = unified_rag.query(query, mode=mode)\n",
    "    print(f\"A: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a50cdc",
   "metadata": {},
   "source": [
    "## 9. Evaluation Metrics\n",
    "\n",
    "Implement metrics to evaluate the system's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a06fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "\n",
      "Mode: basic\n",
      "avg_precision: 0.333\n",
      "avg_recall: 1.000\n",
      "avg_f1: 0.500\n",
      "avg_response_length: 82.500\n",
      "avg_similarity: 1.000\n",
      "\n",
      "Mode: multi_turn\n",
      "avg_precision: 0.333\n",
      "avg_recall: 1.000\n",
      "avg_f1: 0.500\n",
      "avg_response_length: 30.500\n",
      "avg_similarity: 1.000\n",
      "\n",
      "Mode: agentic\n",
      "avg_precision: 0.333\n",
      "avg_recall: 1.000\n",
      "avg_f1: 0.500\n",
      "avg_response_length: 180.500\n",
      "avg_similarity: 0.774\n"
     ]
    }
   ],
   "source": [
    "class RAGEvaluator:\n",
    "    def __init__(self, rag_system: UnifiedBasicRAG):\n",
    "        self.rag_system = rag_system\n",
    "        \n",
    "    def evaluate_retrieval(self, query: str, relevant_docs: List[Document]) -> dict:\n",
    "        \"\"\"Evaluate retrieval performance.\"\"\"\n",
    "        retrieved_docs = self.rag_system.vector_store.search(query)\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        relevant_ids = set(doc.metadata['source_id'] for doc in relevant_docs)\n",
    "        retrieved_ids = set(doc.metadata['source_id'] for doc in retrieved_docs)\n",
    "        \n",
    "        true_positives = len(relevant_ids.intersection(retrieved_ids))\n",
    "        precision = true_positives / len(retrieved_ids) if retrieved_ids else 0\n",
    "        recall = true_positives / len(relevant_ids) if relevant_ids else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    def evaluate_response_quality(self, \n",
    "                                query: str, \n",
    "                                ground_truth: str,\n",
    "                                mode: str = \"basic\") -> dict:\n",
    "        \"\"\"Evaluate response quality.\"\"\"\n",
    "        response = self.rag_system.query(query, mode=mode)\n",
    "        \n",
    "        # Calculate response length\n",
    "        response_length = len(response.split())\n",
    "        \n",
    "        # Simplified similarity score (in practice, use better metrics)\n",
    "        ground_truth_words = set(ground_truth.lower().split())\n",
    "        response_words = set(response.lower().split())\n",
    "        similarity = len(ground_truth_words.intersection(response_words)) / len(ground_truth_words)\n",
    "        \n",
    "        return {\n",
    "            'response_length': response_length,\n",
    "            'similarity_to_ground_truth': similarity\n",
    "        }\n",
    "    \n",
    "    def run_evaluation(self, \n",
    "                      test_cases: List[dict],\n",
    "                      modes: List[str] = [\"basic\", \"multi_turn\", \"agentic\"]) -> dict:\n",
    "        \"\"\"Run full evaluation on test cases.\"\"\"\n",
    "        results = {mode: {'retrieval': [], 'response': []} for mode in modes}\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            query = test_case['query']\n",
    "            ground_truth = test_case['ground_truth']\n",
    "            relevant_docs = test_case['relevant_docs']\n",
    "            \n",
    "            # Evaluate retrieval\n",
    "            retrieval_metrics = self.evaluate_retrieval(query, relevant_docs)\n",
    "            \n",
    "            # Evaluate response for each mode\n",
    "            for mode in modes:\n",
    "                response_metrics = self.evaluate_response_quality(\n",
    "                    query, ground_truth, mode\n",
    "                )\n",
    "                \n",
    "                results[mode]['retrieval'].append(retrieval_metrics)\n",
    "                results[mode]['response'].append(response_metrics)\n",
    "        \n",
    "        # Calculate averages\n",
    "        final_results = {}\n",
    "        for mode in modes:\n",
    "            final_results[mode] = {\n",
    "                'avg_precision': np.mean([m['precision'] for m in results[mode]['retrieval']]),\n",
    "                'avg_recall': np.mean([m['recall'] for m in results[mode]['retrieval']]),\n",
    "                'avg_f1': np.mean([m['f1'] for m in results[mode]['retrieval']]),\n",
    "                'avg_response_length': np.mean([m['response_length'] for m in results[mode]['response']]),\n",
    "                'avg_similarity': np.mean([m['similarity_to_ground_truth'] for m in results[mode]['response']])\n",
    "            }\n",
    "            \n",
    "        return final_results\n",
    "\n",
    "# Create test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        'query': \"Where was Barack Obama born?\",\n",
    "        'ground_truth': \"Barack Obama was born in Hawaii.\",\n",
    "        'relevant_docs': [Document(\n",
    "            text=\"Barack Obama was born in Hawaii.\",\n",
    "            metadata={'source_id': 0}\n",
    "        )]\n",
    "    },\n",
    "    {\n",
    "        'query': \"Where was Michelle Obama born?\",\n",
    "        'ground_truth': \"Michelle Obama was born in Chicago, Illinois.\",\n",
    "        'relevant_docs': [Document(\n",
    "            text=\"Michelle Obama was born in Chicago, Illinois.\",\n",
    "            metadata={'source_id': 1}\n",
    "        )]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "evaluator = RAGEvaluator(unified_rag)\n",
    "evaluation_results = evaluator.run_evaluation(test_cases)\n",
    "\n",
    "# Print results\n",
    "print(\"Evaluation Results:\")\n",
    "for mode, metrics in evaluation_results.items():\n",
    "    print(f\"\\nMode: {mode}\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwenrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
