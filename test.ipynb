{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00522767",
   "metadata": {},
   "source": [
    "# RAG System Implementation with Qwen Models\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) system using Qwen models. The implementation includes:\n",
    "\n",
    "1. Basic Single-Turn RAG\n",
    "2. Feature A: Multi-Turn Search (Optional)\n",
    "3. Feature B: Agentic Workflow (Optional)\n",
    "\n",
    "We'll be using one of the following Qwen models:\n",
    "- Qwen2.5-0.5B-Instruct\n",
    "- Qwen2.5-1.5B-Instruct\n",
    "- Qwen2.5-3B-Instruct\n",
    "- Qwen2.5-7B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb3c00",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "Let's start by installing and importing the required libraries. We'll need:\n",
    "- transformers: For the Qwen model\n",
    "- sentence-transformers: For text embeddings\n",
    "- faiss-cpu/faiss-gpu: For vector storage and similarity search\n",
    "- torch: For deep learning operations\n",
    "- Additional utilities for text processing and data management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a04112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers sentence-transformers faiss-cpu numpy pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4da5d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bee/miniforge3/envs/qwenrag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# set cache\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize Qwen model and tokenizer (we'll use the smallest model for demonstration)\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Initialize sentence transformer for embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098a6f8b",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "In this section, we'll implement the following components:\n",
    "1. Text chunking function to split documents into manageable pieces\n",
    "2. Data loading utilities\n",
    "3. Text preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6fb373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 document chunks\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Class to represent a document chunk.\"\"\"\n",
    "    text: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "def split_text(text: str, chunk_size: int = 200, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"Split text into chunks with overlap.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def load_documents(directory: str) -> List[Document]:\n",
    "    \"\"\"Load documents from a directory.\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # This is a placeholder - implement actual document loading based on your data\n",
    "    # For demonstration, we'll create some sample documents\n",
    "    sample_texts = [\n",
    "        \"Barack Obama was born in Hawaii. He served as the 44th president of the United States.\",\n",
    "        \"Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\",\n",
    "        \"Barack and Michelle Obama met in Chicago and got married in 1992.\",\n",
    "        \"Kathy lives in Lai King\"\n",
    "    ]\n",
    "    \n",
    "    for i, text in enumerate(sample_texts):\n",
    "        chunks = split_text(text)\n",
    "        for chunk in chunks:\n",
    "            doc = Document(\n",
    "                text=chunk,\n",
    "                metadata={'source_id': i, 'source_text': text}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load documents\n",
    "documents = load_documents(\"path/to/your/documents\")\n",
    "print(f\"Loaded {len(documents)} document chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf36d6a",
   "metadata": {},
   "source": [
    "## 3. Vector Store Creation\n",
    "\n",
    "Now we'll create a FAISS vector store to index our document embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "936403b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test search results:\n",
      "- Barack Obama was born in Hawaii. He served as the 44th president of the United States.\n",
      "- Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\n",
      "- Barack and Michelle Obama met in Chicago and got married in 1992.\n"
     ]
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, embedding_dim: int = 384):  # MiniLM-L6-v2 has 384 dimensions\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        self.documents = []\n",
    "        \n",
    "    def add_documents(self, documents: List[Document]):\n",
    "        \"\"\"Add documents to the vector store.\"\"\"\n",
    "        texts = [doc.text for doc in documents]\n",
    "        embeddings = embedding_model.encode(texts, convert_to_tensor=True)\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "        \n",
    "        self.index.add(embeddings)\n",
    "        self.documents.extend(documents)\n",
    "    \n",
    "    def search(self, query: str, k: int = 3) -> List[Document]:\n",
    "        \"\"\"Search for similar documents.\"\"\"\n",
    "        query_embedding = embedding_model.encode([query], convert_to_tensor=True)\n",
    "        query_embedding = query_embedding.cpu().numpy()\n",
    "        \n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        return [self.documents[i] for i in indices[0]]\n",
    "\n",
    "# Create vector store and add documents\n",
    "vector_store = VectorStore()\n",
    "vector_store.add_documents(documents)\n",
    "\n",
    "# Test search\n",
    "results = vector_store.search(\"Where was Barack Obama born?\")\n",
    "print(\"Test search results:\")\n",
    "for doc in results:\n",
    "    print(f\"- {doc.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16b812",
   "metadata": {},
   "source": [
    "## 4. Basic RAG Implementation\n",
    "\n",
    "Now we'll implement the core RAG pipeline components:\n",
    "1. Passage retrieval\n",
    "2. Prompt construction\n",
    "3. Response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a9a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: where did Kathy live\n",
      "Answer: Lai King.\n"
     ]
    }
   ],
   "source": [
    "class BasicRAG:\n",
    "    def __init__(self, model, tokenizer, vector_store):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vector_store = vector_store\n",
    "        self.prompt_instruction = \"\"\"You are a helpful assistant. Reply briefly and only with information explicitly stated in the context that directly answers the question. If you cannot find an answer based on the provided information, say \"I don't have enough information to answer that.\". Use the following examples as a guide for your response style but not context:\n",
    "\n",
    "        Example 1:\n",
    "        Context: The capital of Germany is the city of Berlin. The capital of France is Paris.\n",
    "        Query: What is the capital of Germany?\n",
    "        Answer: The capital of Germany is Berlin.\n",
    "\n",
    "        Example 2:\n",
    "        Context: William Shakespeare was an English playwright. He wrote Romeo and Juliet in the 1590s.\n",
    "        Query: Who wrote Romeo and Juliet?\n",
    "        Answer: Romeo and Juliet was written by William Shakespeare.\n",
    "\n",
    "        Example 3:\n",
    "        Context: BeiJing is in China.\n",
    "        Query: Where is New York?\n",
    "        Answer: I don't have enough information to answer that.\n",
    "\n",
    "        Now, use this context to answer the query:\"\"\"\n",
    "        \n",
    "    def _build_prompt(self, query: str, contexts: List[Document]) -> str:\n",
    "        \"\"\"Build a prompt combining the query and retrieved contexts.\"\"\"\n",
    "        context_str = \"\\n\\n\".join([doc.text for doc in contexts])\n",
    "        \n",
    "        prompt = f\"\"\"{self.prompt_instruction}\n",
    "        Context:\n",
    "        {context_str}\n",
    "\n",
    "        Query: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def generate_response(self, query: str, max_new_tokens: int = 200) -> str:\n",
    "        \"\"\"Generate a response using RAG.\"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        relevant_docs = self.vector_store.search(query)\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = self._build_prompt(query, relevant_docs)\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.3, # less creative answer\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        return response.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# Initialize RAG system\n",
    "rag_system = BasicRAG(model, tokenizer, vector_store)\n",
    "\n",
    "# Test the basic RAG system\n",
    "test_query = \"where did Kathy live\"\n",
    "response = rag_system.generate_response(test_query)\n",
    "print(f\"Question: {test_query}\")\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552f080a",
   "metadata": {},
   "source": [
    "## 5. Query Processing Pipeline\n",
    "\n",
    "Let's implement query processing components that will help with both basic RAG and the optional features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc338576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: What about his wife?\n",
      "Expanded query: What about his wife?\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "class QueryProcessor:\n",
    "    def __init__(self, memory_size: int = 5):\n",
    "        self.conversation_history = deque(maxlen=memory_size) # \n",
    "    \n",
    "    def preprocess_query(self, query: str) -> str:\n",
    "        \"\"\"Clean and normalize the query.\"\"\"\n",
    "        return query.strip()\n",
    "    \n",
    "    def extract_entities(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract key entities from text (simplified version).\"\"\"\n",
    "        # This is a placeholder - in a real system, use NER\n",
    "        words = text.split()\n",
    "        # Simple capitalized words detection\n",
    "        entities = [word for word in words if word[0].isupper()]\n",
    "        return entities\n",
    "    \n",
    "    def expand_query(self, query: str) -> str:\n",
    "        \"\"\"Expand query with relevant context from conversation history.\"\"\"\n",
    "        if not self.conversation_history:\n",
    "            return query\n",
    "            \n",
    "        # Get the last exchange\n",
    "        last_exchange = self.conversation_history[-1]\n",
    "        entities = self.extract_entities(last_exchange[\"query\"] + \" \" + last_exchange[\"response\"])\n",
    "        \n",
    "        # If the query contains pronouns and we have entities, expand it\n",
    "        pronouns = [\"he\", \"she\", \"his\", \"her\", \"their\", \"it\"]\n",
    "        if any(pronoun in query.lower() for pronoun in pronouns) and entities:\n",
    "            expanded_query = f\"{', '.join(entities)}: {query}\"\n",
    "            print(expanded_query)\n",
    "            return expanded_query\n",
    "            \n",
    "        return query\n",
    "    \n",
    "    def add_to_history(self, query: str, response: str):\n",
    "        \"\"\"Add an exchange to conversation history.\"\"\"\n",
    "        self.conversation_history.append({\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"entities\": self.extract_entities(query + \" \" + response)\n",
    "        })\n",
    "\n",
    "# Initialize query processor\n",
    "query_processor = QueryProcessor()\n",
    "\n",
    "# Test query processing\n",
    "test_query = \"What about his wife?\"\n",
    "processed_query = query_processor.preprocess_query(test_query)\n",
    "expanded_query = query_processor.expand_query(processed_query)\n",
    "\n",
    "print(f\"Original query: {test_query}\")\n",
    "print(f\"Expanded query: {expanded_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c3b0e",
   "metadata": {},
   "source": [
    "## 6. Multi-Turn Conversation Handler (Feature A)\n",
    "\n",
    "Implement the multi-turn conversation feature with context management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1b70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing multi-turn conversation:\n",
      "\n",
      "Q: Where was Barack Obama born?\n",
      "A: Barack Obama was born in Hawaii.\n",
      "Where, Barack, Obama, Barack, Obama, Hawaii.: Where does kathy live?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence index must be integer, not 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTesting multi-turn conversation:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     response = \u001b[43mmulti_turn_rag\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mQ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     58\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mA: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mMultiTurnRAG.generate_response\u001b[39m\u001b[34m(self, query, max_new_tokens)\u001b[39m\n\u001b[32m     34\u001b[39m expanded_query = \u001b[38;5;28mself\u001b[39m.query_processor.expand_query(processed_query)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Get response using the expanded query\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Update conversation history\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mself\u001b[39m.query_processor.add_to_history(query, response)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mRAGSystem.generate_response\u001b[39m\u001b[34m(self, query, max_new_tokens)\u001b[39m\n\u001b[32m     42\u001b[39m relevant_docs = \u001b[38;5;28mself\u001b[39m.vector_store.search(query)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Build prompt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m prompt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelevant_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[32m     48\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m.tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mMultiTurnRAG._build_prompt\u001b[39m\u001b[34m(self, query, contexts, include_history)\u001b[39m\n\u001b[32m     10\u001b[39m history_str = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_history \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.query_processor.conversation_history:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     history = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery_processor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconversation_history\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Last 3 exchanges\u001b[39;00m\n\u001b[32m     13\u001b[39m     history_str = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([\n\u001b[32m     14\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh[\u001b[33m'\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m history\n\u001b[32m     16\u001b[39m     ])\n\u001b[32m     17\u001b[39m     history_str = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPrevious conversation:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mhistory_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: sequence index must be integer, not 'slice'"
     ]
    }
   ],
   "source": [
    "class MultiTurnRAG(BasicRAG):\n",
    "    def __init__(self, model, tokenizer, vector_store):\n",
    "        super().__init__(model, tokenizer, vector_store)\n",
    "        self.query_processor = QueryProcessor()\n",
    "        \n",
    "    def _build_prompt(self, query: str, contexts: List[Document], include_history: bool = True) -> str:\n",
    "        \"\"\"Build a prompt with conversation history.\"\"\"\n",
    "        context_str = \"\\n\\n\".join([doc.text for doc in contexts])\n",
    "        \n",
    "        history_str = \"\"\n",
    "        if include_history and self.query_processor.conversation_history:\n",
    "            history = self.query_processor.conversation_history[-3:]  # Last 3 exchanges\n",
    "            history_str = \"\\n\".join([\n",
    "                f\"Query: {h['query']}\\nAnswer: {h['response']}\"\n",
    "                for h in history\n",
    "            ])\n",
    "            history_str = f\"\\nPrevious conversation:\\n{history_str}\\n\"\n",
    "            \n",
    "        prompt = f\"\"\"{self.prompt_instruction}\n",
    "        Context:\n",
    "        {context_str}\n",
    "        {history_str}\n",
    "\n",
    "        Query: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def generate_response(self, query: str, max_new_tokens: int = 200) -> str:\n",
    "        \"\"\"Generate a response with conversation context.\"\"\"\n",
    "        # Process and expand query\n",
    "        processed_query = self.query_processor.preprocess_query(query)\n",
    "        expanded_query = self.query_processor.expand_query(processed_query)\n",
    "        \n",
    "        # Get response using the expanded query\n",
    "        response = super().generate_response(expanded_query)\n",
    "        \n",
    "        # Update conversation history\n",
    "        self.query_processor.add_to_history(query, response)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Initialize multi-turn RAG system\n",
    "multi_turn_rag = MultiTurnRAG(model, tokenizer, vector_store)\n",
    "\n",
    "# Test multi-turn conversation\n",
    "queries = [\n",
    "    \"Where was Barack Obama born?\",\n",
    "    \"Where does kathy live?\",\n",
    "    \"What about his wife, where was she born?\"\n",
    "]\n",
    "\n",
    "print(\"Testing multi-turn conversation:\")\n",
    "for query in queries:\n",
    "    response = multi_turn_rag.generate_response(query)\n",
    "    print(f\"\\nQ: {query}\")\n",
    "    print(f\"A: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165fd767",
   "metadata": {},
   "source": [
    "## 7. Agentic Workflow Components (Feature B)\n",
    "\n",
    "Implement advanced features for improved response quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fcc29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complex Query: Tell me about Barack Obama's early life and his wife's background.\n",
      "Response: Synthesize a coherent answer from the following responses:\n",
      "\n",
      "        Question: Tell me about Barack Obama's early life and his wife's background.\n",
      "\n",
      "        Sub-responses:\n",
      "        You are a helpful assistant. Use the following information to answer the question.\n",
      "        If you cannot find the answer in the provided context, say \"I don't have enough information to answer that.\"\n",
      "\n",
      "        Context:\n",
      "        Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\n",
      "\n",
      "Barack Obama was born in Hawaii. He served as the 44th president of the United States.\n",
      "\n",
      "Barack and Michelle Obama met in Chicago and got married in 1992.\n",
      "        \n",
      "        Question: The first president was George Washington.\n",
      "\n",
      "        Answer: Let me answer based on the provided information. Barack Obama is the first president mentioned in the given context, and he is also referred to as the 44th president of the United States. Therefore, there is not enough information to answer the question about who the first president was without additional context or information from other sources. I don't have enough information to answer that.\n",
      "Therefore, the answer is: I don't have enough information to answer that.\n",
      "You are a helpful assistant. Use the following information to answer the question.\n",
      "        If you cannot find the answer in the provided context, say \"I don't have enough information to answer that.\"\n",
      "\n",
      "        Context:\n",
      "        Barack Obama was born in Hawaii. He served as the 44th president of the United States.\n",
      "\n",
      "Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\n",
      "\n",
      "Barack and Michelle Obama met in Chicago and got married in 1992.\n",
      "        \n",
      "        Question: The second president was John Adams.\n",
      "\n",
      "        Answer: Let me answer based on the provided information. According to the context given, both Barack and Michelle Obama were presidents of the United States. They were born in different states but both served as U.S. presidents. Since they did not meet or get married until after their presidential terms ended, we can conclude that it is unlikely that John Adams would be the second president.\n",
      "\n",
      "Therefore, the correct answer is: I don't have enough information to answer that.\n",
      "The given context does not provide any information about John Adams or his presidency. It only mentions that Barack and Michelle Obama became first ladies after meeting in Chicago and getting married in 1992. There is no information about John Adams' presidency or if he ever became a president.\n",
      "This response directly addresses the question asked by providing an explanation for why there is insufficient information to answer it. It avoids making assumptions about John Adams based on the given context and instead uses the available information from the provided context to make a logical conclusion. The answer also includes a comparison between the two presidents mentioned (Barack Obama and Michelle Obama) to emphasize the lack of information needed to determine which one was the second president. This makes the response clear and concise while still addressing the specific question posed.\n",
      "You are a helpful assistant. Use the following information to answer the question.\n",
      "        If you cannot find the answer in the provided context, say \"I don't have enough information to answer that.\"\n",
      "\n",
      "        Context:\n",
      "        Barack Obama was born in Hawaii. He served as the 44th president of the United States.\n",
      "\n",
      "Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\n",
      "\n",
      "Barack and Michelle Obama met in Chicago and got married in 1992.\n",
      "        \n",
      "        Question: He became president in 1797.\n",
      "\n",
      "        Answer: Let me answer based on the provided information. Barack Obama was born in Hawaii and served as the 44th president of the United States. Therefore, he did not become president in 1797.\n",
      "Therefore, the answer is: I don't have enough information to answer that.\n",
      "The passage only provides information about Barack Obama's birthplace and presidency, but it does not mention any year or presidential election results. Without additional information about when Barack Obama became president, we cannot determine if he did or did not become president in 1797. The given context does not provide sufficient information for answering this specific question.\n",
      "The correct response is: I don't have enough information to answer that.\n",
      "You are a helpful assistant. Use the following information to answer the question.\n",
      "        If you cannot find the answer in the provided context, say \"I don't have enough information to answer that.\"\n",
      "\n",
      "        Context:\n",
      "        Barack Obama was born in Hawaii. He served as the 44th president of the United States.\n",
      "\n",
      "Barack and Michelle Obama met in Chicago and got married in 1992.\n",
      "\n",
      "Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\n",
      "        \n",
      "        Question: He served two terms as president.\n",
      "\n",
      "        Answer: Let me answer based on the provided information. According to the context given, Barack Obama served as the 44th president of the United States from 2009 to 2017. The question specifically asks if he served two terms as president. Since there is no information about him serving more than one term, I can conclude that the answer is:\n",
      "\n",
      "\"I don't have enough information to answer that.\"\n",
      "The correct response would be: Barack Obama did not serve two terms as president; he served only one term, which was for four years. His second term began after his first term ended in 2013. \n",
      "\n",
      "If you need help answering another question using the provided context, please let me know! #AnswerThis\n",
      "The answer is: I don't have enough information to answer that.\n",
      "You are a helpful assistant. Use the following information to answer the question.\n",
      "        If you cannot find the answer in the provided context, say \"I don't have enough information to answer that.\"\n",
      "\n",
      "        Context:\n",
      "        Barack Obama was born in Hawaii. He served as the 44th president of the United States.\n",
      "\n",
      "Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\n",
      "\n",
      "Barack and Michelle Obama met in Chicago and got married in 1992.\n",
      "        \n",
      "        Question: He grew up in New York City.\n",
      "\n",
      "        Answer: Let me answer based on the provided information. Barack Obama was born in Hawaii, which is not located in New York City. Michelle Obama was born in Chicago, which is also not located in New York City. The question asks about where he grew up, but there's no information given about his place of birth or where he lived when he was growing up.\n",
      "Therefore, I don't have enough information to answer that question.\n",
      "The answer is: I don't have enough information to answer that.\n",
      "You are a helpful assistant. Use the following information to answer the question.\n",
      "        If you cannot find the answer in the provided context, say \"I don't have enough information to answer that.\"\n",
      "\n",
      "        Context:\n",
      "        Barack Obama was born in Hawaii. He served as the 44th president of the United States.\n",
      "\n",
      "Barack and Michelle Obama met in Chicago and got married in 1992.\n",
      "\n",
      "Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\n",
      "        \n",
      "        Question: No, he didn't have any siblings; he had a younger brother named John Quincy Adams. \n",
      "\n",
      "        Answer: Let me answer based on the provided information. Barack Obama is the son of Michelle Obama and John Quincy Adams. So there is no mention of him having siblings other than his mother. Therefore, I don't have enough information to answer the question about whether he has any siblings. The correct answer would be:\n",
      "\n",
      "\"I don't have enough information to answer that.\"\n",
      "\n",
      "        Provide a clear, concise answer that addresses the original question. To address the original question accurately, we must consider all relevant details provided in the context. The key points are:\n",
      "\n",
      "- Barack Obama was born in Hawaii.\n",
      "- Michelle Obama was born in Chicago, Illinois.\n",
      "- Both were First Ladies of the United States.\n",
      "- Barack Obama served as the 44th president of the United States.\n",
      "\n",
      "Based on these facts, Barack Obama clearly did not have any siblings and thus did not grow up in New York City like Michelle Obama did. The fact that he had a younger brother named John Quincy Adams further confirms this information. Therefore, the correct answer to the original question is:\n",
      "\n",
      "No, he didn't have any siblings; he had a younger brother named John Quincy Adams. \n",
      "\n",
      "The provided context does not offer any information about his family structure or upbringing, so we cannot use this information to answer the question. However, the core information about his siblings being younger is consistent with the given context.\n"
     ]
    }
   ],
   "source": [
    "class AgenticRAG(MultiTurnRAG):\n",
    "    def __init__(self, model, tokenizer, vector_store):\n",
    "        super().__init__(model, tokenizer, vector_store)\n",
    "    \n",
    "    def _decompose_query(self, query: str) -> List[str]:\n",
    "        \"\"\"Decompose complex queries into simpler sub-queries.\"\"\"\n",
    "        decomposition_prompt = f\"\"\"Break down the following question into simpler sub-questions:\n",
    "        Question: {query}\n",
    "        \n",
    "        Generate sub-questions in a numbered list. Keep it simple.\"\"\"\n",
    "        \n",
    "        inputs = self.tokenizer(decomposition_prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=256)\n",
    "        decomposition = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract sub-questions (simple implementation)\n",
    "        sub_queries = [\n",
    "            line.split(\". \", 1)[1] if \". \" in line else line\n",
    "            for line in decomposition.split(\"\\n\")\n",
    "            if line.strip() and line[0].isdigit()\n",
    "        ]\n",
    "        \n",
    "        return sub_queries\n",
    "    \n",
    "    def _verify_response(self, response: str, context: List[Document]) -> bool:\n",
    "        \"\"\"Verify if the response is supported by the context.\"\"\"\n",
    "        context_str = \"\\n\".join([doc.text for doc in context])\n",
    "        \n",
    "        verification_prompt = f\"\"\"Given the following context and response, determine if the response is fully supported by the context.\n",
    "        \n",
    "        Context: {context_str}\n",
    "        \n",
    "        Response: {response}\n",
    "        \n",
    "        Is the response fully supported by the context? Answer with just 'yes' or 'no'.\"\"\"\n",
    "        \n",
    "        inputs = self.tokenizer(verification_prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=128)\n",
    "        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True).lower()\n",
    "        \n",
    "        return \"yes\" in result\n",
    "    \n",
    "    def generate_response(self, query: str, max_new_tokens: int = 600) -> str:\n",
    "        \"\"\"Generate a response using the agentic workflow.\"\"\"\n",
    "        # 1. Query Planning\n",
    "        sub_queries = self._decompose_query(query)\n",
    "        \n",
    "        # 2. Parallel Retrieval and Response Generation\n",
    "        sub_responses = []\n",
    "        for sub_query in sub_queries:\n",
    "            # Get relevant documents for sub-query\n",
    "            relevant_docs = self.vector_store.search(sub_query)\n",
    "            \n",
    "            # Generate response for sub-query\n",
    "            prompt = self._build_prompt(sub_query, relevant_docs, include_history=False)\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            outputs = self.model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "            sub_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            sub_responses.append(sub_response)\n",
    "        \n",
    "        # 3. Response Synthesis\n",
    "        synthesis_prompt = f\"\"\"Synthesize a coherent answer from the following responses:\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Sub-responses:\n",
    "        {chr(10).join(sub_responses)}\n",
    "        \n",
    "        Provide a clear, concise answer that addresses the original question.\"\"\"\n",
    "        \n",
    "        inputs = self.tokenizer(synthesis_prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        final_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 4. Verification\n",
    "        relevant_docs = self.vector_store.search(query)\n",
    "        if not self._verify_response(final_response, relevant_docs):\n",
    "            final_response += \"\\n[Note: Some details in this response may require verification.]\"\n",
    "        \n",
    "        # Update conversation history\n",
    "        self.query_processor.add_to_history(query, final_response)\n",
    "        \n",
    "        return final_response\n",
    "\n",
    "# Initialize agentic RAG system\n",
    "agentic_rag = AgenticRAG(model, tokenizer, vector_store)\n",
    "\n",
    "# Test complex query\n",
    "complex_query = \"Tell me about Barack Obama's early life and his wife's background.\"\n",
    "response = agentic_rag.generate_response(complex_query)\n",
    "print(f\"\\nComplex Query: {complex_query}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21868f6d",
   "metadata": {},
   "source": [
    "## 8. System Integration\n",
    "\n",
    "Let's create a unified interface that allows switching between different RAG modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e99b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different RAG modes:\n",
      "\n",
      "Mode: basic\n",
      "Q: Where was Barack Obama born?\n",
      "A: According to the given context, Barack Obama was born in Hawaii. The information states:\n",
      "\n",
      "\"Barack Obama was born in Hawaii. He served as the 44th president of the United States.\"\n",
      "\n",
      "So, the correct answer is: Hawaii.\n",
      "I don't have enough information to answer that.\n",
      "Therefore, the response is: I don't have enough information to answer that.\n",
      "\n",
      "Mode: multi_turn\n",
      "Q: What about his wife?\n",
      "A: The passage states that Michelle Obama was born in Chicago, Illinois and she served as First Lady of the United States. It also mentions that Barack Obama was born in Hawaii and he served as the 44th president of the United States. Therefore, it is not possible to determine what about his wife from the given information. I don't have enough information to answer that question.\n",
      "Therefore, my answer is: I don't have enough information to answer that.\n",
      "\n",
      "Mode: agentic\n",
      "Q: Tell me about both Barack and Michelle Obama's early lives.\n",
      "A: Synthesize a coherent answer from the following responses:\n",
      "\n",
      "        Question: Tell me about both Barack and Michelle Obama's early lives.\n",
      "\n",
      "        Sub-responses:\n",
      "        You are a helpful assistant. Use the following information to answer the question.\n",
      "        If you cannot find the answer in the provided context, say \"I don't have enough information to answer that.\"\n",
      "\n",
      "        Context:\n",
      "        Barack Obama was born in Hawaii. He served as the 44th president of the United States.\n",
      "\n",
      "Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\n",
      "\n",
      "Barack and Michelle Obama met in Chicago and got married in 1992.\n",
      "        \n",
      "        Question: Describe your family\n",
      "\n",
      "        Answer: Let me answer based on the provided information. I don't have enough information to answer this question about my family. The given context only provides information about Barack Obama and Michelle Obama's birthplaces and their roles as presidents of the United States. There is no information about my family or any other aspects of it. Therefore, I am unable to provide an accurate description of my family based solely on the provided context. To answer this specific question, additional information would be needed. Can you please provide more details about who you are? I'm here to help with any questions you might have! #familyinformationrequired\n",
      "In summary, the question requires information not available in the provided context. It is necessary to ask for personal information such as name, age, place of birth, etc., to provide a proper response. As requested, I can now proceed to answer the question using the provided context.\n",
      "\n",
      "You are a helpful assistant. Use the following information to answer the question.\n",
      "        If you cannot find the answer in the provided context, say \"I don't have enough information to answer that.\"\n",
      "\n",
      "        Context:\n",
      "        Barack and Michelle Obama met in Chicago and got married in 1992.\n",
      "\n",
      "Barack Obama was born in Hawaii. He served as the 44th president of the United States.\n",
      "\n",
      "Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\n",
      "        \n",
      "        Question: Explain how you got involved with politics\n",
      "\n",
      "        Answer: Let me answer based on the provided information. I don't have enough information to directly answer this question about how I got involved with politics.\n",
      "        Information used: The given context provides biographical details about Barack Obama and Michelle Obama, including their birthplaces and political roles. However, it does not provide any information about my involvement in politics or how I became interested in politics.\n",
      "        Response: In order to answer this question, additional information about my personal experiences and interests would be necessary. Without this type of information, it is impossible for me to accurately answer whether I got involved with politics or not. The available information only tells us about my family's background and my role in the political process, but not about my own political beliefs or actions. Therefore, I do not have sufficient information to answer this specific question about my involvement in politics. To provide an accurate response, I would need more specific details about my personal history and political activities. Please let me know if there is anything else I can help you with! #PoliticalAwareness\n",
      "In summary, while I do not have enough information to directly answer the question about my involvement with politics, I am aware of my family's background and political experience. My goal is to provide accurate and relevant information to those who may be interested in learning more about my life and career. Is there anything else I can assist you with? #PoliticalAwareness\n",
      "\n",
      "You are a helpful assistant. Use the following information to answer the question.\n",
      "        If you cannot find the answer in the provided context, say \"I don't have enough information to answer that.\"\n",
      "\n",
      "        Context:\n",
      "        Michelle Obama was born in Chicago, Illinois. She served as First Lady of the United States.\n",
      "\n",
      "Barack Obama was born in Hawaii. He served as the 44th president of the United States.\n",
      "\n",
      "Barack and Michelle Obama met in Chicago and got married in 1992.\n",
      "        \n",
      "        Question: What else did you do before becoming President?\n",
      "\n",
      "        Answer: Let me answer based on the provided information. Barack Obama did not serve as the 44th president of the United States. Therefore, he had nothing else to do before becoming President. I don't have enough information to answer that question. \n",
      "\n",
      "So the final answer is: I don't have enough information to answer that.\n",
      "\n",
      "        Provide a clear, concise answer that addresses the original question. The answer should include all the key points from the provided information, but also avoid repeating information already present in the text. The answer should be grammatically correct and easy to understand. #Obama\n"
     ]
    }
   ],
   "source": [
    "class UnifiedBasicRAG:\n",
    "    def __init__(self, model_name: str = \"Qwen/Qwen2.5-0.5B-Instruct\"):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.vector_store = VectorStore()\n",
    "        \n",
    "        # Initialize different RAG modes\n",
    "        self.basic_rag = BasicRAG(self.model, self.tokenizer, self.vector_store)\n",
    "        self.multi_turn_rag = MultiTurnRAG(self.model, self.tokenizer, self.vector_store)\n",
    "        self.agentic_rag = AgenticRAG(self.model, self.tokenizer, self.vector_store)\n",
    "        \n",
    "    def add_documents(self, documents: List[Document]):\n",
    "        \"\"\"Add documents to the knowledge base.\"\"\"\n",
    "        self.vector_store.add_documents(documents)\n",
    "    \n",
    "    def query(self, \n",
    "             query: str, \n",
    "             mode: str = \"basic\",\n",
    "             max_new_tokens: int = 200) -> str:\n",
    "        \"\"\"\n",
    "        Query the RAG system using specified mode.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's question\n",
    "            mode: One of [\"basic\", \"multi_turn\", \"agentic\"]\n",
    "            max_new_tokens: Maximum length of the generated response\n",
    "        \"\"\"\n",
    "        if mode == \"basic\":\n",
    "            return self.basic_rag.generate_response(query, max_new_tokens)\n",
    "        elif mode == \"multi_turn\":\n",
    "            return self.multi_turn_rag.generate_response(query, max_new_tokens)\n",
    "        elif mode == \"agentic\":\n",
    "            return self.agentic_rag.generate_response(query, max_new_tokens)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "# Initialize unified system\n",
    "unified_rag = UnifiedBasicRAG()\n",
    "unified_rag.add_documents(documents)\n",
    "\n",
    "# Test different modes\n",
    "test_queries = [\n",
    "    (\"Where was Barack Obama born?\", \"basic\"),\n",
    "    (\"What about his wife?\", \"multi_turn\"),\n",
    "    (\"Tell me about both Barack and Michelle Obama's early lives.\", \"agentic\")\n",
    "]\n",
    "\n",
    "print(\"Testing different RAG modes:\")\n",
    "for query, mode in test_queries:\n",
    "    print(f\"\\nMode: {mode}\")\n",
    "    print(f\"Q: {query}\")\n",
    "    response = unified_rag.query(query, mode=mode)\n",
    "    print(f\"A: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a50cdc",
   "metadata": {},
   "source": [
    "## 9. Evaluation Metrics\n",
    "\n",
    "Implement metrics to evaluate the system's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a06fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "\n",
      "Mode: basic\n",
      "avg_precision: 0.333\n",
      "avg_recall: 1.000\n",
      "avg_f1: 0.500\n",
      "avg_response_length: 82.500\n",
      "avg_similarity: 1.000\n",
      "\n",
      "Mode: multi_turn\n",
      "avg_precision: 0.333\n",
      "avg_recall: 1.000\n",
      "avg_f1: 0.500\n",
      "avg_response_length: 30.500\n",
      "avg_similarity: 1.000\n",
      "\n",
      "Mode: agentic\n",
      "avg_precision: 0.333\n",
      "avg_recall: 1.000\n",
      "avg_f1: 0.500\n",
      "avg_response_length: 180.500\n",
      "avg_similarity: 0.774\n"
     ]
    }
   ],
   "source": [
    "class RAGEvaluator:\n",
    "    def __init__(self, rag_system: UnifiedBasicRAG):\n",
    "        self.rag_system = rag_system\n",
    "        \n",
    "    def evaluate_retrieval(self, query: str, relevant_docs: List[Document]) -> dict:\n",
    "        \"\"\"Evaluate retrieval performance.\"\"\"\n",
    "        retrieved_docs = self.rag_system.vector_store.search(query)\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        relevant_ids = set(doc.metadata['source_id'] for doc in relevant_docs)\n",
    "        retrieved_ids = set(doc.metadata['source_id'] for doc in retrieved_docs)\n",
    "        \n",
    "        true_positives = len(relevant_ids.intersection(retrieved_ids))\n",
    "        precision = true_positives / len(retrieved_ids) if retrieved_ids else 0\n",
    "        recall = true_positives / len(relevant_ids) if relevant_ids else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    def evaluate_response_quality(self, \n",
    "                                query: str, \n",
    "                                ground_truth: str,\n",
    "                                mode: str = \"basic\") -> dict:\n",
    "        \"\"\"Evaluate response quality.\"\"\"\n",
    "        response = self.rag_system.query(query, mode=mode)\n",
    "        \n",
    "        # Calculate response length\n",
    "        response_length = len(response.split())\n",
    "        \n",
    "        # Simplified similarity score (in practice, use better metrics)\n",
    "        ground_truth_words = set(ground_truth.lower().split())\n",
    "        response_words = set(response.lower().split())\n",
    "        similarity = len(ground_truth_words.intersection(response_words)) / len(ground_truth_words)\n",
    "        \n",
    "        return {\n",
    "            'response_length': response_length,\n",
    "            'similarity_to_ground_truth': similarity\n",
    "        }\n",
    "    \n",
    "    def run_evaluation(self, \n",
    "                      test_cases: List[dict],\n",
    "                      modes: List[str] = [\"basic\", \"multi_turn\", \"agentic\"]) -> dict:\n",
    "        \"\"\"Run full evaluation on test cases.\"\"\"\n",
    "        results = {mode: {'retrieval': [], 'response': []} for mode in modes}\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            query = test_case['query']\n",
    "            ground_truth = test_case['ground_truth']\n",
    "            relevant_docs = test_case['relevant_docs']\n",
    "            \n",
    "            # Evaluate retrieval\n",
    "            retrieval_metrics = self.evaluate_retrieval(query, relevant_docs)\n",
    "            \n",
    "            # Evaluate response for each mode\n",
    "            for mode in modes:\n",
    "                response_metrics = self.evaluate_response_quality(\n",
    "                    query, ground_truth, mode\n",
    "                )\n",
    "                \n",
    "                results[mode]['retrieval'].append(retrieval_metrics)\n",
    "                results[mode]['response'].append(response_metrics)\n",
    "        \n",
    "        # Calculate averages\n",
    "        final_results = {}\n",
    "        for mode in modes:\n",
    "            final_results[mode] = {\n",
    "                'avg_precision': np.mean([m['precision'] for m in results[mode]['retrieval']]),\n",
    "                'avg_recall': np.mean([m['recall'] for m in results[mode]['retrieval']]),\n",
    "                'avg_f1': np.mean([m['f1'] for m in results[mode]['retrieval']]),\n",
    "                'avg_response_length': np.mean([m['response_length'] for m in results[mode]['response']]),\n",
    "                'avg_similarity': np.mean([m['similarity_to_ground_truth'] for m in results[mode]['response']])\n",
    "            }\n",
    "            \n",
    "        return final_results\n",
    "\n",
    "# Create test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        'query': \"Where was Barack Obama born?\",\n",
    "        'ground_truth': \"Barack Obama was born in Hawaii.\",\n",
    "        'relevant_docs': [Document(\n",
    "            text=\"Barack Obama was born in Hawaii.\",\n",
    "            metadata={'source_id': 0}\n",
    "        )]\n",
    "    },\n",
    "    {\n",
    "        'query': \"Where was Michelle Obama born?\",\n",
    "        'ground_truth': \"Michelle Obama was born in Chicago, Illinois.\",\n",
    "        'relevant_docs': [Document(\n",
    "            text=\"Michelle Obama was born in Chicago, Illinois.\",\n",
    "            metadata={'source_id': 1}\n",
    "        )]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "evaluator = RAGEvaluator(unified_rag)\n",
    "evaluation_results = evaluator.run_evaluation(test_cases)\n",
    "\n",
    "# Print results\n",
    "print(\"Evaluation Results:\")\n",
    "for mode, metrics in evaluation_results.items():\n",
    "    print(f\"\\nMode: {mode}\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwenrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
